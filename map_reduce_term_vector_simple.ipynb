{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01GqCJRjMuWV"
      },
      "source": [
        "# Map Reduce\n",
        "\n",
        "This notebook is performing map reduce in a simplified manner in Python. Distribution of compute to different nodes is not done here; the purpose rather is to explore how to implement a map or reduce function, assuming that the functionality is provided akin to the libraries mentioned in [Dean and Ghemawat](https://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf).\n",
        "\n",
        "\n",
        "This notebook comprises a section defining identity mappers and reducers, along with a `run` method which you may change if necessary. An intermediate sort function is also provided.\n",
        "\n",
        "Implement the `mapper` and `reducer` in the Term Vectors section, and use the run cell as provided.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWORW_JjMuWd",
        "outputId": "1f9eaa25-b6b2-4055-d602-cce344c977f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "from itertools import groupby\n",
        "from operator import itemgetter\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "%config Completer.use_jedi = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GpFPGZ0bMuWh"
      },
      "outputs": [],
      "source": [
        "# Empty MAPPER\n",
        "def mapper(key, value):\n",
        "\n",
        " terms = value.split()  # Simple whitespace tokenization\n",
        " for term in terms:\n",
        "        # Yield each term with a count of 1\n",
        "\n",
        "        yield (term, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "SWGF8dYmMuWi"
      },
      "outputs": [],
      "source": [
        "# Empty REDUCER\n",
        "\n",
        "def reducer(key , list_value):\n",
        "\n",
        "    total_count = sum(list_value)\n",
        "\n",
        "    yield (key, total_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2NauENHMuWk",
        "outputId": "723d2c27-545d-44e8-d58c-1fb14107dba3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:8: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "<>:8: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "<ipython-input-5-eefcc1524b63>:8: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  if not (word is '' or word in stopwords.words('english')):\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "def cleaner(line):\n",
        "    # lowercase all words and get alphabetical char only and keeping\n",
        "    # apostrophe for time being\n",
        "    words = re.findall(r'[a-z\\']+' , line.lower())\n",
        "    for word in words :\n",
        "        # we will omit apostrophe ' s assuming users won't type them in a search\n",
        "        word = word.replace(\"'\" , '')\n",
        "        if not (word is '' or word in stopwords.words('english')):\n",
        "            yield word\n",
        "\n",
        "\n",
        "\n",
        "def intermediate_sort(data):\n",
        "    \"\"\"\n",
        "    collect by key\n",
        "    \"\"\"\n",
        "    data = sorted ( data )\n",
        "    return [(k, list(tuple(zip(*g))[1])) for k, g  in groupby(data , itemgetter(0))]\n",
        "\n",
        "\n",
        "\n",
        "def run(sources_dict):\n",
        "    \"\"\"\n",
        "    Since we are focusing on the mapper and reducer functions here we have to\n",
        "    provide the boiler plate code that a MapReduce library typically would . This\n",
        "    function does that in a simple way (we ignore distributing it for now).\n",
        "    : param sources_dict : dictionary where (key,fqfilename), for example ('doc_id','/home/fileX')\n",
        "    \"\"\"\n",
        "    map_result =[]\n",
        "    reduce_result =[]\n",
        "    # open the files and apply map to each of them ( could be done in parallel ,\n",
        "    # but we prefer to keep it simple ) .\n",
        "    for k , v in sources_dict.items():\n",
        "        # do map per source\n",
        "        # this could happen in its own process / worker typically\n",
        "        f = open(v, 'r')\n",
        "        map_result += list(mapper(k, f.read()))\n",
        "        f.close()\n",
        "#         ::alt\n",
        "#          with open(v, 'r') as f:\n",
        "#             for line in f.readlines():\n",
        "#                 map_result += list(mapper(k, line))\n",
        "    # this would be written to disk in the original paradigm ,\n",
        "    # but we keep it in memory for ease of use\n",
        "    intermediate_result = intermediate_sort(map_result)\n",
        "    # now that the data has been ' collected ' and grouped by key it can be handed\n",
        "    # to the reducers . They would run over partitions or chunks usually , but we\n",
        "    # will just iterate through the keys we have and call them\n",
        "    for elem in intermediate_result:\n",
        "        reduce_result.append(list(reducer(elem [0], elem [1])))\n",
        "    return map_result, intermediate_result, reduce_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAkNCGStMuWl",
        "outputId": "8cfa8353-8665-4178-a5e3-b6711dcf6c43"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[(':', 2)],\n",
              " [('D1', 1)],\n",
              " [('D2', 1)],\n",
              " [('cat', 1)],\n",
              " [('dog', 1)],\n",
              " [('log', 1)],\n",
              " [('mat', 1)],\n",
              " [('on', 2)],\n",
              " [('sat', 2)],\n",
              " [('the', 4)]]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# EXAMPLE\n",
        "!mkdir -p input/\n",
        "!echo -e 'D1 : the cat sat on the mat' > input/d1.txt\n",
        "!echo -e 'D2 : the dog sat on the log' > input/d2.txt\n",
        "\n",
        "_, _, res = run({'D1': 'input/d1.txt' , 'D2': 'input/d2.txt'})\n",
        "\n",
        "res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDMcUJMOMuWm"
      },
      "source": [
        "# Term Vector\n",
        "\n",
        "The paper states:\n",
        "\n",
        "> Term-Vector per Host: A term vector summarizes the\n",
        "most important words that occur in a document or a set\n",
        "of documents as a list of 〈word, frequency〉 pairs. The\n",
        "map function emits a 〈hostname, term vector〉\n",
        "pair for each input document (where the hostname is\n",
        "extracted from the URL of the document). The re-\n",
        "duce function is passed all per-document term vectors\n",
        "for a given host. It adds these term vectors together,\n",
        "throwing away infrequent terms, and then emits a final\n",
        "〈hostname, term vector〉 pair.\n",
        "\n",
        "As for\n",
        "\n",
        "> throwing away infrequent terms\n",
        "\n",
        "Write your code in such a way that only terms occurring at least twice are retained.\n",
        "\n",
        "Hint:\n",
        "  * Consider how they use the word 'frequency' elsewhere in the paper.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "3FdHebrVMuWn"
      },
      "outputs": [],
      "source": [
        "# your mapper\n",
        "def mapper(key, value):\n",
        "    yield (key, value)\n",
        "\n",
        "def reducer(key , list_value):\n",
        "    yield (key, list_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "M61lDBvCMuWo",
        "outputId": "f247004d-b006-43fd-fc46-6c6e401f371d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'page1.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-4874c6a5529c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'www.somesite.com/page/1'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'page1.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'www.somesite.com/page/2'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'page2.txt'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-eefcc1524b63>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(sources_dict)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# do map per source\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# this could happen in its own process / worker typically\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mmap_result\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'page1.txt'"
          ]
        }
      ],
      "source": [
        "x, y, res = run({'www.somesite.com/page/1': 'page1.txt', 'www.somesite.com/page/2': 'page2.txt'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4iUg8nzpMuWo"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "bdt-a3",
      "language": "python",
      "name": "bdt-a3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}